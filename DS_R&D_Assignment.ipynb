{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical exercise - Data scientist intern @ Giskard\n",
    "\n",
    "Hi! As part of our recruitment process, we’d like you to complete the following technical test in 10 days. Once you finish the exercise, you can send your notebook or share your code repository by email (matteo@giskard.ai). If you want to share a private GitHub repository, make sure you give read access to `mattbit`.\n",
    "\n",
    "If you have problems running the notebook, get in touch with Matteo at matteo@giskard.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.14.5)\n",
      "Requirement already satisfied: transformers in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.33.1)\n",
      "Requirement already satisfied: torch in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.1+cu118)\n",
      "Requirement already satisfied: giskard>=2.0.0b in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.0b17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\loic\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from giskard>=2.0.0b) (2.2.1)\n",
      "Requirement already satisfied: zstandard>=0.10.0 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from giskard>=2.0.0b) (0.21.0)\n",
      "Requirement already satisfied: mlflow-skinny>=2 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from giskard>=2.0.0b) (2.6.0)\n",
      "Requirement already satisfied: mixpanel>=4.4.0 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from giskard>=2.0.0b) (4.10.0)\n",
      "Requirement already satisfied: pydantic<2,>=1.7 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from giskard>=2.0.0b) (1.10.12)\n",
      "Requirement already satisfied: setuptools<68.0.0,>=39.1.0 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from giskard>=2.0.0b) (65.5.0)\n",
      "Requirement already satisfied: langdetect>=1.0.9 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from giskard>=2.0.0b) (1.0.9)\n",
      "Requirement already satisfied: chardet in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from giskard>=2.0.0b) (5.2.0)\n",
      "Requirement already satisfied: markdown in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from giskard>=2.0.0b) (3.4.4)\n",
      "Requirement already satisfied: requests-toolbelt>=0.9.1 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from giskard>=2.0.0b) (1.0.0)\n",
      "Requirement already satisfied: stomp-py>=8.1.0 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from giskard>=2.0.0b) (8.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: six in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langdetect>=1.0.9->giskard>=2.0.0b) (1.16.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mixpanel>=4.4.0->giskard>=2.0.0b) (1.26.13)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\loic\\appdata\\roaming\\python\\python311\\site-packages (from mlflow-skinny>=2->giskard>=2.0.0b) (8.1.7)\n",
      "Requirement already satisfied: databricks-cli<1,>=0.8.7 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow-skinny>=2->giskard>=2.0.0b) (0.17.7)\n",
      "Requirement already satisfied: entrypoints<1 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow-skinny>=2->giskard>=2.0.0b) (0.4)\n",
      "Requirement already satisfied: gitpython<4,>=2.1.0 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow-skinny>=2->giskard>=2.0.0b) (3.1.35)\n",
      "Requirement already satisfied: protobuf<5,>=3.12.0 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow-skinny>=2->giskard>=2.0.0b) (3.20.3)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow-skinny>=2->giskard>=2.0.0b) (6.8.0)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from mlflow-skinny>=2->giskard>=2.0.0b) (0.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: docopt<0.7.0,>=0.6.2 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stomp-py>=8.1.0->giskard>=2.0.0b) (0.6.2)\n",
      "Requirement already satisfied: websocket-client<2.0.0,>=1.2.3 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stomp-py>=8.1.0->giskard>=2.0.0b) (1.6.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\loic\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Requirement already satisfied: pyjwt>=1.7.0 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from databricks-cli<1,>=0.8.7->mlflow-skinny>=2->giskard>=2.0.0b) (2.8.0)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from databricks-cli<1,>=0.8.7->mlflow-skinny>=2->giskard>=2.0.0b) (3.2.2)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from databricks-cli<1,>=0.8.7->mlflow-skinny>=2->giskard>=2.0.0b) (0.9.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gitpython<4,>=2.1.0->mlflow-skinny>=2->giskard>=2.0.0b) (4.0.10)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow-skinny>=2->giskard>=2.0.0b) (3.16.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\loic\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow-skinny>=2->giskard>=2.0.0b) (5.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas scikit-learn datasets transformers torch \"giskard>=2.0.0b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Code review\n",
    "\n",
    "Your fellow intern is working on securing our API and wrote some code to generate secure tokens. You have been asked to review their code and make sure it is secure and robust. Can you spot the problem and write a short feedback?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "ALPHABET = \"abcdefghijklmnopqrstuvxyz0123456789\"\n",
    "\n",
    "\n",
    "def generate_secret_key(size: int = 20):\n",
    "    \"\"\"Generates a cryptographically secure random token.\"\"\"\n",
    "    token = \"\".join(random.choice(ALPHABET) for _ in range(size))\n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There could be a small problem with the fact that the letter \"w\" is missing from the ALPHABET.\n",
    "\n",
    "Alternatively using the random module in this circumstance is not advisable. Someone could find the inital seed that was used to generate the token, and remake the token.\n",
    "The secrets module could be used instead.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import secrets\n",
    "\n",
    "ALPHABET = \"abcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "\n",
    "def generate_secret_key(size: int = 20):\n",
    "    \"\"\"Generates a cryptographically secure random token.\"\"\"\n",
    "    token = ''.join(secrets.choice(ALPHABET) for _ in range(size))\n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: High dimensions\n",
    "\n",
    "Matteo, our ML researcher, is struggling with a dataset of 40-dimensional points. He’s sure there are some clusters in there, but he does not know how many. Can you help him find the correct number of clusters in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like there are 11 clusters.\n"
     ]
    }
   ],
   "source": [
    "# Silhouette Score\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "x = np.load(\"points_1.npy\")\n",
    "\n",
    "limit = int((x.shape[0]//2)**0.5)\n",
    "scoreList = []\n",
    "\n",
    "for k in range(2, limit+1):\n",
    "    model = KMeans(n_clusters=k, n_init=10)\n",
    "    model.fit(x)\n",
    "    pred = model.predict(x)\n",
    "    score = silhouette_score(x, pred)\n",
    "    scoreList.append(score)\n",
    "    \n",
    "print(\"It looks like there are {} clusters.\".format(scoreList.index(max(scoreList))+2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matteo is grateful for how you helped him with the cluster finding, and he has another problem for you. He has another high-dimensional dataset, but he thinks that those points could be represented in a lower dimensional space. Can you help him determine how many dimensions would be enough to well represent the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.0712989   8.69575874  6.18598189  4.22223949  0.93416246  0.58704625\n",
      "  0.48948339  0.46406602  0.40220908  0.39272047  0.35745547  0.34110437\n",
      "  0.31023658  0.29937102  0.28709567  0.28278631  0.2671307   0.25265965\n",
      "  0.24244703  0.22468491  0.21587419  0.20995291  0.20514762  0.19230804\n",
      "  0.18833865  0.17475562  0.16809787  0.16193516  0.14479163  0.14299042\n",
      "  0.13401393  0.1208438   0.10968856  0.10352973  0.09752006  0.09012386\n",
      "  0.0839506   0.07465291  0.0671299   0.0444562 ]\n",
      "[0.30148069 0.51865726 0.67315216 0.77860259 0.8019333  0.81659478\n",
      " 0.82881963 0.84040968 0.85045485 0.86026304 0.86919049 0.87770958\n",
      " 0.88545773 0.89293452 0.90010474 0.90716733 0.91383892 0.92014909\n",
      " 0.92620421 0.93181571 0.93720717 0.94245074 0.94757431 0.9523772\n",
      " 0.95708096 0.96144548 0.96564372 0.96968805 0.97330422 0.97687541\n",
      " 0.98022241 0.98324048 0.98597995 0.98856561 0.99100117 0.99325201\n",
      " 0.99534868 0.99721314 0.99888971 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x = np.load(\"points_2.npy\")\n",
    "xScaled = StandardScaler().fit_transform(x)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(xScaled)\n",
    "\n",
    "print(pca.explained_variance_)\n",
    "print(np.cumsum(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the first 5 principal components are enough to describe around 80% of the data. \n",
    "If we want to describe around 95% of the data we would need 24 principal components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=5)\n",
    "# xReduced = pca.fit_transform(xScaled) \n",
    "\n",
    "# print(xReduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Mad GPT\n",
    "\n",
    "Matteo is a good guy but he is a bit messy: he fine-tuned a GPT-2 model, but it seems that something went wrong during the process and the model became obsessed with early Romantic literature.\n",
    "\n",
    "Could you check how the model would continue a sentence starting with “Ty”? Could you recover the logit of the next best token? And its probability?\n",
    "\n",
    "You can get the model from the HuggingFace Hub as `mattbit/gpt2wb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tyger Tyger, burning bright, \n",
      "In the forests of the night; \n",
      "What immortal hand or eye, \n",
      "Could frame thy fearful symmetry?\n",
      "In what distant deeps or skies. \n",
      "Burnt the fire of thine eyes?\n",
      "On what wings dare he aspire?\n",
      "What the hand, dare seize the fire?\n",
      "And what shoulder, & what art,\n",
      "Could twist the sinews of thy heart?\n",
      "And when thy heart began to beat.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mattbit/gpt2wb\")\n",
    "\n",
    "inputIds = tokenizer.encode(\"Ty\", return_tensors=\"pt\")\n",
    "output = model.generate(inputIds, max_length=100)\n",
    "outputText = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(outputText)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit : -22.899076461791992, output text: gers, probability : 0.0013438640162348747\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = tokenizer(\"Ty\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "softmaxOutput = torch.nn.functional.softmax(outputs.logits, dim=2)\n",
    "\n",
    "topValues, topIndices = torch.topk(softmaxOutput, k=2, dim=2)\n",
    "outputText = tokenizer.decode(topIndices[0][0][1], skip_special_tokens=True)\n",
    "\n",
    "print(\"logit : {}, output text: {}, probability : {}\".format(outputs.logits[0][0][topIndices[0][0][1].item()] ,outputText, topValues[0][0][1].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Not bad reviews\n",
    "\n",
    "\n",
    "We trained a random forest model to predict if a film review is positive or negative. Here is the training code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Accuracy: 0.7431192660550459\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Load training data\n",
    "train_data = datasets.load_dataset(\"sst2\", split=\"train[:20000]\").to_pandas()\n",
    "valid_data = datasets.load_dataset(\"sst2\", split=\"validation\").to_pandas()\n",
    "\n",
    "# Prepare model\n",
    "with open(\"stopwords.txt\", \"r\") as f:\n",
    "    stopwords = [w.strip() for w in f.readlines()]\n",
    "\n",
    "preprocessor = TfidfVectorizer(stop_words=stopwords, max_features=5000, lowercase=False) #look into this \n",
    "classifier = RandomForestClassifier(n_estimators=400, n_jobs=-1)\n",
    "\n",
    "model = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", classifier)])\n",
    "\n",
    "# Train\n",
    "X = train_data.sentence\n",
    "y = train_data.label\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\n",
    "    \"Training complete.\",\n",
    "    \"Accuracy:\",\n",
    "    model.score(valid_data.sentence, valid_data.label),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, it works quite well, but we noticed it has some problems with reviews containing negations, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good']\n",
      "['bad']\n",
      "['bad' 'not']\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\loic\\OneDrive\\Documents\\school\\stage\\ds-Giskard-intern-test\\DS_R&D_Assignment.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/loic/OneDrive/Documents/school/stage/ds-Giskard-intern-test/DS_R%26D_Assignment.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m testPreprocesser\u001b[39m.\u001b[39mfit_transform([\u001b[39m\"\u001b[39m\u001b[39mthis movie is not bad at all!\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/loic/OneDrive/Documents/school/stage/ds-Giskard-intern-test/DS_R%26D_Assignment.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(testPreprocesser\u001b[39m.\u001b[39mget_feature_names_out())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/loic/OneDrive/Documents/school/stage/ds-Giskard-intern-test/DS_R%26D_Assignment.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39massert\u001b[39;00m model\u001b[39m.\u001b[39mpredict([\u001b[39m\"\u001b[39m\u001b[39mthis movie is not bad at all!\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m==\u001b[39m [\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/loic/OneDrive/Documents/school/stage/ds-Giskard-intern-test/DS_R%26D_Assignment.ipynb#X24sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# WHOOPS! this ↓ is predicted as negative?! why?\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/loic/OneDrive/Documents/school/stage/ds-Giskard-intern-test/DS_R%26D_Assignment.ipynb#X24sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m testPreprocesser\u001b[39m.\u001b[39mfit_transform([\u001b[39m\"\u001b[39m\u001b[39mthis movie is not perfect, but very good!\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Class labels are:\n",
    "# 1 = Positive, 0 = Negative\n",
    "testPreprocesser = TfidfVectorizer(stop_words=stopwords, max_features=5000, lowercase=False)\n",
    "\n",
    "# this returns positive, that’s right!\n",
    "testPreprocesser.fit_transform([\"this movie is good\"])\n",
    "print(testPreprocesser.get_feature_names_out())\n",
    "assert model.predict([\"this movie is good\"]) == [1]\n",
    "\n",
    "# negative! bingo!\n",
    "testPreprocesser.fit_transform([\"this movie is bad\"])\n",
    "print(testPreprocesser.get_feature_names_out())\n",
    "assert model.predict([\"this movie is bad\"]) == [0]\n",
    "\n",
    "# WHOOPS! this ↓ is predicted as negative?! uhm…\n",
    "testPreprocesser.fit_transform([\"this movie is not bad at all!\"])\n",
    "print(testPreprocesser.get_feature_names_out())\n",
    "assert model.predict([\"this movie is not bad at all!\"]) == [1]\n",
    "\n",
    "# WHOOPS! this ↓ is predicted as negative?! why?\n",
    "testPreprocesser.fit_transform([\"this movie is not perfect, but very good!\"])\n",
    "print(testPreprocesser.get_feature_names_out())\n",
    "assert model.predict([\"this movie is not perfect, but very good!\"]) == [1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you help us understand what is going on? Do you have any idea on how to fix it?\n",
    "You can edit the code above.\n",
    "\n",
    "One of the problems comes with the model in and of itself. Decision trees, and by extension, random forests, work by taking one element and determining how it influences the outcome, moving down the list in reducing order of importance. Negation needs context to be understood, which is hard to implement in this type of model.\n",
    "\n",
    "If we look at how the sentences are preprocessed, we're selecting the words that could represent the most information and taking into account their frequency. We could try to take into account the position of the words in the sentence.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Model weaknesses\n",
    "\n",
    "\n",
    "The Giskard python library provides an automatic scanner to find weaknesses and vulnerabilities in ML models.\n",
    "\n",
    "Using this tool, could you identify some issues in the movie classification model above? Can you propose hypotheses about what is causing these issues?\n",
    "\n",
    "Then, choose one of the issues you just found and try to improve the model to mitigate or resolve it — just one, no need to spend the whole weekend over it!\n",
    "\n",
    "You can find a quickstart here: https://docs.giskard.ai/en/latest/getting-started/quickstart.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
